{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow.ipynb",
      "provenance": [],
      "mount_file_id": "1tEXGRX-bOOVLJUknsgQ8YBCGHNeWBoDg",
      "authorship_tag": "ABX9TyPRo40IFAsLSVPhbrqqpYKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zolzayakh/Dive-into-coding/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Question 1] Looking back on the scratch**\n",
        "\n"
      ],
      "metadata": {
        "id": "zRnjfAFUKBfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIsZItvyJqLI",
        "outputId": "a1eae2d3-836b-492e-bb42-9378aba1afc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.0241, val_loss : 67.6860, acc : 0.375\n",
            "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
            "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
            "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
            "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
            "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
            "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
            "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
            "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
            "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
            "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
            "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
            "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
            "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
            "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
            "Epoch 15, loss : 0.8304, val_loss : 5.7681, acc : 0.375\n",
            "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
            "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
            "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
            "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
            "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
            "Epoch 21, loss : 0.5738, val_loss : 3.1371, acc : 0.500\n",
            "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
            "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
            "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
            "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
            "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
            "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
            "Epoch 28, loss : 0.3326, val_loss : 1.3725, acc : 0.750\n",
            "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
            "Epoch 30, loss : 0.2851, val_loss : 1.1453, acc : 0.750\n",
            "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
            "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
            "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
            "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
            "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
            "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
            "Epoch 37, loss : 0.1815, val_loss : 0.4516, acc : 0.812\n",
            "Epoch 38, loss : 0.1735, val_loss : 0.4015, acc : 0.812\n",
            "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
            "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
            "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
            "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
            "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
            "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
            "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
            "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
            "Epoch 47, loss : 0.1139, val_loss : 0.1098, acc : 0.875\n",
            "Epoch 48, loss : 0.1091, val_loss : 0.0947, acc : 0.938\n",
            "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
            "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
            "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 53, loss : 0.0915, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
            "Epoch 55, loss : 0.0875, val_loss : 0.0598, acc : 0.938\n",
            "Epoch 56, loss : 0.0790, val_loss : 0.0376, acc : 1.000\n",
            "Epoch 57, loss : 0.0871, val_loss : 0.0935, acc : 0.938\n",
            "Epoch 58, loss : 0.0723, val_loss : 0.0862, acc : 0.938\n",
            "Epoch 59, loss : 0.0936, val_loss : 0.1731, acc : 0.938\n",
            "Epoch 60, loss : 0.0693, val_loss : 0.1648, acc : 0.938\n",
            "Epoch 61, loss : 0.1047, val_loss : 0.2714, acc : 0.938\n",
            "Epoch 62, loss : 0.0711, val_loss : 0.1687, acc : 0.938\n",
            "Epoch 63, loss : 0.1072, val_loss : 0.3122, acc : 0.938\n",
            "Epoch 64, loss : 0.0738, val_loss : 0.1538, acc : 0.938\n",
            "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
            "Epoch 66, loss : 0.0757, val_loss : 0.1524, acc : 0.938\n",
            "Epoch 67, loss : 0.1082, val_loss : 0.3200, acc : 0.938\n",
            "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
            "Epoch 69, loss : 0.1114, val_loss : 0.3153, acc : 0.938\n",
            "Epoch 70, loss : 0.0808, val_loss : 0.1600, acc : 0.938\n",
            "Epoch 71, loss : 0.1139, val_loss : 0.2888, acc : 0.938\n",
            "Epoch 72, loss : 0.0824, val_loss : 0.1424, acc : 0.938\n",
            "Epoch 73, loss : 0.1110, val_loss : 0.2300, acc : 0.938\n",
            "Epoch 74, loss : 0.0791, val_loss : 0.1061, acc : 0.938\n",
            "Epoch 75, loss : 0.1002, val_loss : 0.1768, acc : 0.938\n",
            "Epoch 76, loss : 0.0719, val_loss : 0.0579, acc : 0.938\n",
            "Epoch 77, loss : 0.0810, val_loss : 0.0948, acc : 0.938\n",
            "Epoch 78, loss : 0.0610, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 79, loss : 0.0601, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 80, loss : 0.0514, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
            "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
            "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 84, loss : 0.0430, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 86, loss : 0.0400, val_loss : 0.0493, acc : 0.938\n",
            "Epoch 87, loss : 0.0430, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 88, loss : 0.0374, val_loss : 0.0669, acc : 0.938\n",
            "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
            "Epoch 90, loss : 0.0361, val_loss : 0.0710, acc : 0.938\n",
            "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
            "Epoch 92, loss : 0.0349, val_loss : 0.0693, acc : 0.938\n",
            "Epoch 93, loss : 0.0411, val_loss : 0.0055, acc : 1.000\n",
            "Epoch 94, loss : 0.0337, val_loss : 0.0678, acc : 0.938\n",
            "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
            "Epoch 96, loss : 0.0326, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 97, loss : 0.0376, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 98, loss : 0.0316, val_loss : 0.0499, acc : 0.938\n",
            "Epoch 99, loss : 0.0349, val_loss : 0.0138, acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.test.gpu_device_name() \n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "# データセットの読み込み\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Iris.csv\")\n",
        "\n",
        "# データフレームから条件抽出\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# ラベルを数値に変換\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# load network structure                            \n",
        "logits = example_net(X)\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# estimation result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Problem 3] Create a model of Iris using all three types of objective variables**"
      ],
      "metadata": {
        "id": "tl-QvouoKTMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Iris.csv\")\n",
        "\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y[y == \"Iris-setosa\"] = 2\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1mba_HHKO9u",
        "outputId": "802670ce-06f9-4650-fc5c-54ce568b1944"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def iris_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# load network structure                            \n",
        "logits = iris_net(X)\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # softmax\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# estimation result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) #argmax\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SbJ9FoiLF7r",
        "outputId": "dc65193b-54ce-4887-c5e6-362b0dd6c84c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 4.2144, val_loss : 32.9527, acc : 0.167\n",
            "Epoch 1, loss : 2.6672, val_loss : 18.4617, acc : 0.000\n",
            "Epoch 2, loss : 1.4687, val_loss : 11.3532, acc : 0.417\n",
            "Epoch 3, loss : 0.8378, val_loss : 6.2571, acc : 0.667\n",
            "Epoch 4, loss : 0.4912, val_loss : 4.5932, acc : 0.542\n",
            "Epoch 5, loss : 0.3636, val_loss : 3.4168, acc : 0.542\n",
            "Epoch 6, loss : 0.2337, val_loss : 2.5563, acc : 0.625\n",
            "Epoch 7, loss : 0.1482, val_loss : 1.8462, acc : 0.708\n",
            "Epoch 8, loss : 0.0958, val_loss : 1.3645, acc : 0.708\n",
            "Epoch 9, loss : 0.0659, val_loss : 1.0763, acc : 0.833\n",
            "Epoch 10, loss : 0.0463, val_loss : 0.8686, acc : 0.875\n",
            "Epoch 11, loss : 0.0346, val_loss : 0.7448, acc : 0.875\n",
            "Epoch 12, loss : 0.0275, val_loss : 0.7041, acc : 0.875\n",
            "Epoch 13, loss : 0.0227, val_loss : 0.6614, acc : 0.875\n",
            "Epoch 14, loss : 0.0195, val_loss : 0.6152, acc : 0.875\n",
            "Epoch 15, loss : 0.0172, val_loss : 0.5870, acc : 0.875\n",
            "Epoch 16, loss : 0.0155, val_loss : 0.5744, acc : 0.875\n",
            "Epoch 17, loss : 0.0142, val_loss : 0.5606, acc : 0.875\n",
            "Epoch 18, loss : 0.0132, val_loss : 0.5445, acc : 0.875\n",
            "Epoch 19, loss : 0.0124, val_loss : 0.5300, acc : 0.875\n",
            "Epoch 20, loss : 0.0117, val_loss : 0.5173, acc : 0.875\n",
            "Epoch 21, loss : 0.0111, val_loss : 0.5052, acc : 0.875\n",
            "Epoch 22, loss : 0.0106, val_loss : 0.4930, acc : 0.875\n",
            "Epoch 23, loss : 0.0101, val_loss : 0.4819, acc : 0.833\n",
            "Epoch 24, loss : 0.0097, val_loss : 0.4747, acc : 0.833\n",
            "Epoch 25, loss : 0.0092, val_loss : 0.4677, acc : 0.833\n",
            "Epoch 26, loss : 0.0088, val_loss : 0.4573, acc : 0.833\n",
            "Epoch 27, loss : 0.0085, val_loss : 0.4442, acc : 0.833\n",
            "Epoch 28, loss : 0.0082, val_loss : 0.4340, acc : 0.833\n",
            "Epoch 29, loss : 0.0079, val_loss : 0.4241, acc : 0.833\n",
            "Epoch 30, loss : 0.0077, val_loss : 0.4100, acc : 0.833\n",
            "Epoch 31, loss : 0.0075, val_loss : 0.3994, acc : 0.833\n",
            "Epoch 32, loss : 0.0073, val_loss : 0.3930, acc : 0.833\n",
            "Epoch 33, loss : 0.0072, val_loss : 0.3850, acc : 0.833\n",
            "Epoch 34, loss : 0.0070, val_loss : 0.3726, acc : 0.833\n",
            "Epoch 35, loss : 0.0069, val_loss : 0.3651, acc : 0.833\n",
            "Epoch 36, loss : 0.0068, val_loss : 0.3605, acc : 0.833\n",
            "Epoch 37, loss : 0.0067, val_loss : 0.3535, acc : 0.833\n",
            "Epoch 38, loss : 0.0066, val_loss : 0.3472, acc : 0.833\n",
            "Epoch 39, loss : 0.0066, val_loss : 0.3421, acc : 0.875\n",
            "Epoch 40, loss : 0.0065, val_loss : 0.3379, acc : 0.875\n",
            "Epoch 41, loss : 0.0064, val_loss : 0.3345, acc : 0.875\n",
            "Epoch 42, loss : 0.0063, val_loss : 0.3285, acc : 0.875\n",
            "Epoch 43, loss : 0.0062, val_loss : 0.3254, acc : 0.875\n",
            "Epoch 44, loss : 0.0061, val_loss : 0.3244, acc : 0.875\n",
            "Epoch 45, loss : 0.0060, val_loss : 0.3227, acc : 0.875\n",
            "Epoch 46, loss : 0.0059, val_loss : 0.3198, acc : 0.875\n",
            "Epoch 47, loss : 0.0058, val_loss : 0.3188, acc : 0.875\n",
            "Epoch 48, loss : 0.0057, val_loss : 0.3182, acc : 0.875\n",
            "Epoch 49, loss : 0.0056, val_loss : 0.3159, acc : 0.875\n",
            "Epoch 50, loss : 0.0056, val_loss : 0.3153, acc : 0.875\n",
            "Epoch 51, loss : 0.0055, val_loss : 0.3153, acc : 0.875\n",
            "Epoch 52, loss : 0.0054, val_loss : 0.3137, acc : 0.875\n",
            "Epoch 53, loss : 0.0053, val_loss : 0.3128, acc : 0.875\n",
            "Epoch 54, loss : 0.0053, val_loss : 0.3126, acc : 0.875\n",
            "Epoch 55, loss : 0.0052, val_loss : 0.3120, acc : 0.875\n",
            "Epoch 56, loss : 0.0051, val_loss : 0.3113, acc : 0.875\n",
            "Epoch 57, loss : 0.0051, val_loss : 0.3108, acc : 0.875\n",
            "Epoch 58, loss : 0.0050, val_loss : 0.3105, acc : 0.875\n",
            "Epoch 59, loss : 0.0049, val_loss : 0.3102, acc : 0.875\n",
            "Epoch 60, loss : 0.0049, val_loss : 0.3099, acc : 0.875\n",
            "Epoch 61, loss : 0.0048, val_loss : 0.3098, acc : 0.875\n",
            "Epoch 62, loss : 0.0048, val_loss : 0.3093, acc : 0.875\n",
            "Epoch 63, loss : 0.0047, val_loss : 0.3102, acc : 0.875\n",
            "Epoch 64, loss : 0.0047, val_loss : 0.3115, acc : 0.875\n",
            "Epoch 65, loss : 0.0046, val_loss : 0.3108, acc : 0.875\n",
            "Epoch 66, loss : 0.0046, val_loss : 0.3105, acc : 0.875\n",
            "Epoch 67, loss : 0.0045, val_loss : 0.3112, acc : 0.875\n",
            "Epoch 68, loss : 0.0045, val_loss : 0.3116, acc : 0.875\n",
            "Epoch 69, loss : 0.0045, val_loss : 0.3119, acc : 0.875\n",
            "Epoch 70, loss : 0.0044, val_loss : 0.3122, acc : 0.875\n",
            "Epoch 71, loss : 0.0044, val_loss : 0.3126, acc : 0.875\n",
            "Epoch 72, loss : 0.0043, val_loss : 0.3127, acc : 0.875\n",
            "Epoch 73, loss : 0.0043, val_loss : 0.3129, acc : 0.875\n",
            "Epoch 74, loss : 0.0043, val_loss : 0.3131, acc : 0.875\n",
            "Epoch 75, loss : 0.0042, val_loss : 0.3144, acc : 0.875\n",
            "Epoch 76, loss : 0.0042, val_loss : 0.3141, acc : 0.875\n",
            "Epoch 77, loss : 0.0041, val_loss : 0.3140, acc : 0.875\n",
            "Epoch 78, loss : 0.0041, val_loss : 0.3147, acc : 0.875\n",
            "Epoch 79, loss : 0.0041, val_loss : 0.3147, acc : 0.875\n",
            "Epoch 80, loss : 0.0040, val_loss : 0.3147, acc : 0.917\n",
            "Epoch 81, loss : 0.0040, val_loss : 0.3151, acc : 0.917\n",
            "Epoch 82, loss : 0.0039, val_loss : 0.3163, acc : 0.917\n",
            "Epoch 83, loss : 0.0039, val_loss : 0.3149, acc : 0.917\n",
            "Epoch 84, loss : 0.0039, val_loss : 0.3139, acc : 0.917\n",
            "Epoch 85, loss : 0.0038, val_loss : 0.3136, acc : 0.917\n",
            "Epoch 86, loss : 0.0038, val_loss : 0.3141, acc : 0.917\n",
            "Epoch 87, loss : 0.0038, val_loss : 0.3130, acc : 0.917\n",
            "Epoch 88, loss : 0.0037, val_loss : 0.3133, acc : 0.917\n",
            "Epoch 89, loss : 0.0037, val_loss : 0.3132, acc : 0.917\n",
            "Epoch 90, loss : 0.0037, val_loss : 0.3122, acc : 0.917\n",
            "Epoch 91, loss : 0.0036, val_loss : 0.3120, acc : 0.917\n",
            "Epoch 92, loss : 0.0036, val_loss : 0.3120, acc : 0.917\n",
            "Epoch 93, loss : 0.0036, val_loss : 0.3126, acc : 0.917\n",
            "Epoch 94, loss : 0.0035, val_loss : 0.3115, acc : 0.917\n",
            "Epoch 95, loss : 0.0035, val_loss : 0.3111, acc : 0.917\n",
            "Epoch 96, loss : 0.0035, val_loss : 0.3122, acc : 0.917\n",
            "Epoch 97, loss : 0.0034, val_loss : 0.3108, acc : 0.917\n",
            "Epoch 98, loss : 0.0034, val_loss : 0.3104, acc : 0.917\n",
            "Epoch 99, loss : 0.0034, val_loss : 0.3107, acc : 0.917\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Question 4] Create a House Prices model**"
      ],
      "metadata": {
        "id": "a_CxR1nELRR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation of House Price dataset\n",
        "dataset_path =\"/content/drive/MyDrive/Colab Notebooks/train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKvx-J1uLQsP",
        "outputId": "878e3b1b-fc49-4954-e88c-cfea0f7339d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def house_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# load network structure                            \n",
        "logits = house_net(X)\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=logits) # mean squared error\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "MSE = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y, predictions=logits))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, mse = sess.run([loss_op, MSE], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, mse = sess.run([loss_op, MSE], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, mse : {:.3f}\".format(epoch, total_loss, val_loss, mse))\n",
        "    test_acc = sess.run(MSE, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_mse : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMItiP0iLfns",
        "outputId": "544c10f3-486b-409b-a5d2-706bb3701486"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 2.4392, val_loss : 1.7546, mse : 1.755\n",
            "Epoch 1, loss : 0.1241, val_loss : 1.0228, mse : 1.023\n",
            "Epoch 2, loss : 0.0883, val_loss : 0.7482, mse : 0.748\n",
            "Epoch 3, loss : 0.0679, val_loss : 0.5908, mse : 0.591\n",
            "Epoch 4, loss : 0.0548, val_loss : 0.4944, mse : 0.494\n",
            "Epoch 5, loss : 0.0462, val_loss : 0.4235, mse : 0.423\n",
            "Epoch 6, loss : 0.0400, val_loss : 0.3801, mse : 0.380\n",
            "Epoch 7, loss : 0.0354, val_loss : 0.3579, mse : 0.358\n",
            "Epoch 8, loss : 0.0319, val_loss : 0.3448, mse : 0.345\n",
            "Epoch 9, loss : 0.0289, val_loss : 0.3276, mse : 0.328\n",
            "Epoch 10, loss : 0.0261, val_loss : 0.3058, mse : 0.306\n",
            "Epoch 11, loss : 0.0238, val_loss : 0.2813, mse : 0.281\n",
            "Epoch 12, loss : 0.0216, val_loss : 0.2547, mse : 0.255\n",
            "Epoch 13, loss : 0.0196, val_loss : 0.2311, mse : 0.231\n",
            "Epoch 14, loss : 0.0180, val_loss : 0.2106, mse : 0.211\n",
            "Epoch 15, loss : 0.0166, val_loss : 0.1944, mse : 0.194\n",
            "Epoch 16, loss : 0.0155, val_loss : 0.1818, mse : 0.182\n",
            "Epoch 17, loss : 0.0145, val_loss : 0.1699, mse : 0.170\n",
            "Epoch 18, loss : 0.0137, val_loss : 0.1593, mse : 0.159\n",
            "Epoch 19, loss : 0.0131, val_loss : 0.1516, mse : 0.152\n",
            "Epoch 20, loss : 0.0125, val_loss : 0.1453, mse : 0.145\n",
            "Epoch 21, loss : 0.0119, val_loss : 0.1402, mse : 0.140\n",
            "Epoch 22, loss : 0.0115, val_loss : 0.1350, mse : 0.135\n",
            "Epoch 23, loss : 0.0112, val_loss : 0.1294, mse : 0.129\n",
            "Epoch 24, loss : 0.0110, val_loss : 0.1253, mse : 0.125\n",
            "Epoch 25, loss : 0.0109, val_loss : 0.1222, mse : 0.122\n",
            "Epoch 26, loss : 0.0107, val_loss : 0.1198, mse : 0.120\n",
            "Epoch 27, loss : 0.0107, val_loss : 0.1170, mse : 0.117\n",
            "Epoch 28, loss : 0.0107, val_loss : 0.1136, mse : 0.114\n",
            "Epoch 29, loss : 0.0107, val_loss : 0.1113, mse : 0.111\n",
            "Epoch 30, loss : 0.0108, val_loss : 0.1107, mse : 0.111\n",
            "Epoch 31, loss : 0.0110, val_loss : 0.1134, mse : 0.113\n",
            "Epoch 32, loss : 0.0112, val_loss : 0.1175, mse : 0.117\n",
            "Epoch 33, loss : 0.0114, val_loss : 0.1267, mse : 0.127\n",
            "Epoch 34, loss : 0.0116, val_loss : 0.1401, mse : 0.140\n",
            "Epoch 35, loss : 0.0118, val_loss : 0.1588, mse : 0.159\n",
            "Epoch 36, loss : 0.0120, val_loss : 0.1797, mse : 0.180\n",
            "Epoch 37, loss : 0.0123, val_loss : 0.2008, mse : 0.201\n",
            "Epoch 38, loss : 0.0123, val_loss : 0.2237, mse : 0.224\n",
            "Epoch 39, loss : 0.0122, val_loss : 0.2426, mse : 0.243\n",
            "Epoch 40, loss : 0.0121, val_loss : 0.2668, mse : 0.267\n",
            "Epoch 41, loss : 0.0120, val_loss : 0.2820, mse : 0.282\n",
            "Epoch 42, loss : 0.0118, val_loss : 0.2967, mse : 0.297\n",
            "Epoch 43, loss : 0.0115, val_loss : 0.3221, mse : 0.322\n",
            "Epoch 44, loss : 0.0113, val_loss : 0.3260, mse : 0.326\n",
            "Epoch 45, loss : 0.0110, val_loss : 0.3284, mse : 0.328\n",
            "Epoch 46, loss : 0.0107, val_loss : 0.3378, mse : 0.338\n",
            "Epoch 47, loss : 0.0103, val_loss : 0.3272, mse : 0.327\n",
            "Epoch 48, loss : 0.0100, val_loss : 0.3291, mse : 0.329\n",
            "Epoch 49, loss : 0.0097, val_loss : 0.3343, mse : 0.334\n",
            "Epoch 50, loss : 0.0096, val_loss : 0.3394, mse : 0.339\n",
            "Epoch 51, loss : 0.0095, val_loss : 0.3439, mse : 0.344\n",
            "Epoch 52, loss : 0.0094, val_loss : 0.3321, mse : 0.332\n",
            "Epoch 53, loss : 0.0091, val_loss : 0.3067, mse : 0.307\n",
            "Epoch 54, loss : 0.0092, val_loss : 0.2757, mse : 0.276\n",
            "Epoch 55, loss : 0.0092, val_loss : 0.2568, mse : 0.257\n",
            "Epoch 56, loss : 0.0092, val_loss : 0.2267, mse : 0.227\n",
            "Epoch 57, loss : 0.0092, val_loss : 0.1992, mse : 0.199\n",
            "Epoch 58, loss : 0.0089, val_loss : 0.1808, mse : 0.181\n",
            "Epoch 59, loss : 0.0084, val_loss : 0.1712, mse : 0.171\n",
            "Epoch 60, loss : 0.0081, val_loss : 0.1611, mse : 0.161\n",
            "Epoch 61, loss : 0.0079, val_loss : 0.1532, mse : 0.153\n",
            "Epoch 62, loss : 0.0078, val_loss : 0.1510, mse : 0.151\n",
            "Epoch 63, loss : 0.0076, val_loss : 0.1519, mse : 0.152\n",
            "Epoch 64, loss : 0.0073, val_loss : 0.1563, mse : 0.156\n",
            "Epoch 65, loss : 0.0072, val_loss : 0.1639, mse : 0.164\n",
            "Epoch 66, loss : 0.0072, val_loss : 0.1768, mse : 0.177\n",
            "Epoch 67, loss : 0.0072, val_loss : 0.1881, mse : 0.188\n",
            "Epoch 68, loss : 0.0074, val_loss : 0.1968, mse : 0.197\n",
            "Epoch 69, loss : 0.0075, val_loss : 0.1971, mse : 0.197\n",
            "Epoch 70, loss : 0.0075, val_loss : 0.1989, mse : 0.199\n",
            "Epoch 71, loss : 0.0077, val_loss : 0.1990, mse : 0.199\n",
            "Epoch 72, loss : 0.0076, val_loss : 0.1861, mse : 0.186\n",
            "Epoch 73, loss : 0.0074, val_loss : 0.1771, mse : 0.177\n",
            "Epoch 74, loss : 0.0076, val_loss : 0.1918, mse : 0.192\n",
            "Epoch 75, loss : 0.0078, val_loss : 0.1960, mse : 0.196\n",
            "Epoch 76, loss : 0.0078, val_loss : 0.2011, mse : 0.201\n",
            "Epoch 77, loss : 0.0076, val_loss : 0.1817, mse : 0.182\n",
            "Epoch 78, loss : 0.0075, val_loss : 0.1748, mse : 0.175\n",
            "Epoch 79, loss : 0.0076, val_loss : 0.1818, mse : 0.182\n",
            "Epoch 80, loss : 0.0080, val_loss : 0.2032, mse : 0.203\n",
            "Epoch 81, loss : 0.0081, val_loss : 0.1979, mse : 0.198\n",
            "Epoch 82, loss : 0.0075, val_loss : 0.1775, mse : 0.177\n",
            "Epoch 83, loss : 0.0073, val_loss : 0.1583, mse : 0.158\n",
            "Epoch 84, loss : 0.0076, val_loss : 0.1528, mse : 0.153\n",
            "Epoch 85, loss : 0.0084, val_loss : 0.1815, mse : 0.181\n",
            "Epoch 86, loss : 0.0085, val_loss : 0.1981, mse : 0.198\n",
            "Epoch 87, loss : 0.0084, val_loss : 0.1992, mse : 0.199\n",
            "Epoch 88, loss : 0.0079, val_loss : 0.1825, mse : 0.183\n",
            "Epoch 89, loss : 0.0076, val_loss : 0.1762, mse : 0.176\n",
            "Epoch 90, loss : 0.0078, val_loss : 0.1656, mse : 0.166\n",
            "Epoch 91, loss : 0.0079, val_loss : 0.1844, mse : 0.184\n",
            "Epoch 92, loss : 0.0080, val_loss : 0.1940, mse : 0.194\n",
            "Epoch 93, loss : 0.0076, val_loss : 0.1824, mse : 0.182\n",
            "Epoch 94, loss : 0.0072, val_loss : 0.1598, mse : 0.160\n",
            "Epoch 95, loss : 0.0072, val_loss : 0.1498, mse : 0.150\n",
            "Epoch 96, loss : 0.0075, val_loss : 0.1551, mse : 0.155\n",
            "Epoch 97, loss : 0.0076, val_loss : 0.1689, mse : 0.169\n",
            "Epoch 98, loss : 0.0077, val_loss : 0.1705, mse : 0.170\n",
            "Epoch 99, loss : 0.0077, val_loss : 0.1745, mse : 0.174\n",
            "test_mse : 0.157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Problem 5] Create an MNIST model**"
      ],
      "metadata": {
        "id": "DY5jHxztLkIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data preparation for MNIST \n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1MXYifvLjHp",
        "outputId": "35bb89ed-9c69-4e65-b9d0-eabfd416d256"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 64 # changed the batch size and epoch\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "# Determine the form of arguments passed to the computation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "# load network structure                            \n",
        "logits = iris_net(X) # we use same network\n",
        "\n",
        "# loss function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # softmax\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# estimation result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) #argmax\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de_UHJv7LqdS",
        "outputId": "320242dc-c02e-4b70-e7fc-f790a7a58111"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.5205, val_loss : 26.9693, acc : 0.094\n",
            "Epoch 1, loss : 0.1704, val_loss : 4.7829, acc : 0.093\n",
            "Epoch 2, loss : 0.0580, val_loss : 3.0302, acc : 0.064\n",
            "Epoch 3, loss : 0.0434, val_loss : 2.6435, acc : 0.046\n",
            "Epoch 4, loss : 0.0394, val_loss : 2.4985, acc : 0.043\n",
            "Epoch 5, loss : 0.0378, val_loss : 2.4220, acc : 0.041\n",
            "Epoch 6, loss : 0.0370, val_loss : 2.3854, acc : 0.043\n",
            "Epoch 7, loss : 0.0365, val_loss : 2.3705, acc : 0.042\n",
            "Epoch 8, loss : 0.0363, val_loss : 2.3647, acc : 0.042\n",
            "Epoch 9, loss : 0.0362, val_loss : 2.3534, acc : 0.044\n",
            "test_acc : 0.038\n"
          ]
        }
      ]
    }
  ]
}